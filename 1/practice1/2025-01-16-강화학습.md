# **[실습1] 결정론적 시뮬레이션 환경에 Q-Learning 적용하기**

이번 실습에서는 간단한 결정론적 시뮬레이션 환경을 활용하여 Q-Learning을 직접 구현해보도록 하겠습니다.

지시사항을 따라 `[실습1] 결정론적 환경에서의 Q Learning.ipynb` 노트북 파일에 적절한 코드를 작성해주세요.

코드를 완성해야 할 부분은 코드 상에 `TODO` 로 시작하는 주석에 있습니다. 해당 부분에 있는 `None` 을 올바른 코드로 대체해주세요.

## **지시사항**

아래 지시사항을 참고하여 코드의 `None` 부분을 채워 코드를 완성합니다.

1. Q-Learning 학습을 진행하는 함수를 완성합니다.
   - **1A**. 현재 상태(state)를 바탕으로 Q-Table에서 행동을 결정해서 `action` 변수에 할당하는 코드를 완성해주세요.
   - **1B**. 오차를 계산해서 `error` 변수에 할당하는 코드를 완성해주세요.
   - **1C**. 오차를 기반으로 Q-Table을 업데이트하는 코드를 완성해주세요.

### Tips!

아래 함수는 문제를 해결하는데 도움이 될 수 있는 함수입니다.

`np.argmax(list)` : list 변수에 있는 리스트에서 가장 큰 값이 있는 index를 반환합니다

```
list = np.array([1, 4, 2, 3, 7])
print(np.argmax(list)) # 4를 출력합니다.
Copy
```

`np.max(list)` : list 변수에 있는 리스트에서 가장 큰 값을 반환합니다

```
list = np.array([1, 4, 2, 3, 7])
print(np.max(list)) # 7을 출력합니다.
```



# **[실습2] 비결정론적 시뮬레이션 환경에 Q-Learning 적용하기**

이번 실습에서는 간단한 비결정론적 시뮬레이션 환경을 활용하여 Q-Learning을 직접 구현해보도록 하겠습니다.

비결정론적인 환경이란, 에이전트가 특정한 행동을 취했을 때 그에 따른 다음 상태와 보상이 **하나로 결정되지 않고** 확률에 따라 정해지는 환경을 의미합니다.

이번 실습은 Taxi-v3 환경과 달리 목적지에 도착했을 때만 보상 혹은 페널티가 주어지는 환경입니다. 주어지는 보상 혹은 페널티가 적어짐에 따라 Agent가 환경을 탐험할 요인이 상대적으로 줄어들게 됩니다. 이러한 환경에서 탐험을 촉진하기 위해 **Epsilon-greedy를 적용한 Q-Learning을 구현해 보겠습니다.**

지시사항을 따라 `[실습2] 비결정론적 환경에서의 Q Learning.ipynb` 노트북 파일에 적절한 코드를 작성해주세요.

코드를 완성해야 할 부분은 코드 상에 `TODO` 로 시작하는 주석에 있습니다. 해당 부분에 있는 `None` 을 올바른 코드로 대체해주세요.

## **지시사항**

아래 지시사항을 참고하여 코드의 `None` 부분을 채워 코드를 완성합니다.

1. Epsilon-greedy를 적용한 Q-Learning 학습을 진행하는 함수를 완성합니다.

- **1A**. `epsilon` 값이 epsilon_end 보다 작아지지 않도록 하는 코드를 완성해주세요.
- **1B**. `epsilon` 확률로 랜덤한 액션을 선택하고, (1 - `epsilon`) 확률로 Q-Table을 바탕으로 액션을 선택하는 조건문을 `condition` 변수에 할당해주세요
- **1C**. 현재 상태(state)를 바탕으로 Q-Table에서 행동을 결정해서 `action` 변수에 할당하는 코드를 완성해주세요.
- **1D**. 오차를 계산해서 `error` 변수에 할당하는 코드를 완성해주세요.
- **1E**. 오차를 기반으로 Q-Table을 업데이트하는 코드를 완성해주세요.
- **1F**. Epsilon 값을 업데이트하는 코드를 짜주세요. (epsilon ← epsilon x (1 - epsilon 감가율))

### Tips!

아래 함수는 문제를 해결하는데 도움이 될 수 있는 함수입니다.

`random.random()`: 0 이상 1 미만의 랜덤한 실수를 생성해서 반환합니다.

`np.argmax(list)` : list 변수에 있는 리스트에서 가장 큰 값이 있는 index를 반환합니다

```
list = np.array([1, 4, 2, 3, 7])
print(np.argmax(list)) # 4를 출력합니다.
Copy
```

`max(a, b)` : a, b중 더 큰 값을 반환합니다. (np.max와 다른 함수입니다)

```
print(max(3, 0)) # 3을 출력합니다.
```