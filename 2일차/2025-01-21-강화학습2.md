# **[실습1] Deep Q-Network 구현 실습**

이번 실습에서는 단순 Q-Learning으로는 대응할 수 없는, State가 매우 많은 시뮬레이션 환경에 Deep Q-Network를 구현 및 적용해보도록 하겠습니다.

지시사항을 따라 `[실습1] Deep Q Network 구현.ipynb` 노트북 파일에 적절한 코드를 작성해주세요.

코드를 완성해야 할 부분은 코드 상에 `TODO` 로 시작하는 주석에 있습니다. 해당 부분에 있는 `None` 을 올바른 코드로 대체해주세요.

## **지시사항**

아래 지시사항과 코드 및 주석의 설명을 참고하여 코드의 `None` 부분을 채워 코드를 완성합니다.

1. Deep Q Network를 적용한 Agent의 `train` 메서드를 완성합니다.

   - 이 메서드의 역할은 Q Network를 1회 학습시키는 것입니다.
   - **1A**. Replay Buffer에서 self.batch_size 개수만큼 랜덤한 데이터를 샘플링해서 `batch` 변수에 저장하는 코드를 완성해주세요.
   - **1B**. Q 함수를 학습하기 위해, 예상 Q 값을 계산하는 코드를 완성해주세요.

2. Deep Q Network를 학습시키는 `train` 함수를 완성합니다.

   - 코드 상단에서 랜덤한 행동을 수행하는 Agent를 시뮬레이팅하는 코드를 참고할 수 있습니다.

   - 2A

     . 생성된 행동을 환경에 적용하고, 환경이 반환하는 상태

      

     xt+1

     , 보상

      

     rtr

     , 종료 여부

      

     dt

     를 저장하는 코드를 작성해주세요.

     - 각각 `next_raw_state`, `reward`, `done` 변수에 저장하면 됩니다.

   - 2B

     . 반환된

      

     xt+1

      

     를 StateManager에 추가하여 전처리된 상태

      

     st+1

      

     을 얻는 코드를 작성해주세요.

     - DQNAgent class의 `add_raw_state` 메서드와 `get_current_state` 메서드를 활용해주세요.

   - **2C**. (st,at,rt,st+1,dt)(*st*,*at*,*rt*,*st*+1,*dt*) 5-tuple을 Replay Buffer에 저장하는 코드를 작성해주세요.



# **[실습2] Policy Gradient, Actor-Critic 구현 실습**

이번 실습에서는 State 뿐만 아니라 **가능한 Action 역시 연속적인** 시뮬레이션 환경에서 Policy Gradient와 Actor-Critic 기반 Agent를 각각 구현 및 적용해보도록 하겠습니다.

가능한 Action이 연속적이라는 의미는 가능한 행동이 {왼쪽, 오른쪽} 과 같이 서로 구분되는 값으로 표현되는 것이 아니라, -3.0 ~ 3.0 사이의 모든 실수 값 등 연속적인 값으로 표현되는 것을 의미합니다.

지시사항을 따라 `[실습2] Policy Gradient, Actor-Critic 구현.ipynb` 노트북 파일에 적절한 코드를 작성해주세요.

코드를 완성해야 할 부분은 코드 상에 `TODO` 로 시작하는 주석에 있습니다. 해당 부분에 있는 `None` 을 올바른 코드로 대체해주세요.

## ***\*지시사항\****

아래 지시사항과 코드 및 ***\*주석의 설명\****을 참고하여 코드의 `None` 부분을 채워 코드를 완성합니다.

1. Policy Gradient 알고리즘을 적용한 Agent의 `act` 메서드와 `train` 메서드를 완성합니다.

- - `act` 메서드는 Policy 신경망을 활용하여 상태 *s**t* 에서 행동 *a**t*를 샘플링합니다.
  - `train` 메서드는 한 Episode가 종료될 때 마다 목표함수 *J*(*θ*)를 계산하고, 목표 함수의 Gradient를 활용해 Policy 신경망을 학습합니다.
  - ***\*1A\****. `act` 메서드에서 Policy가 출력한 평균 (`mu`) 과 표준 편차 (`std`)를 따르는 정규 분포를 생성하는 코드를 완성해주세요.
  - ***\*1B\****. `train` 메서드에서 각 Step에서의 누적 보상을 계산하는 코드를 완성해주세요.
  - - 가장 마지막 Step부터 거꾸로 내려가며 보상을 계산하는 방식입니다.
  - ***\*1C\****. `train` 메서드에서 목적 함수를 계산하는 코드를 완성해주세요.

1. Advantage Actor Critic 알고리즘을 적용한 Agent의 `train` 메서드를 완성합니다.

- - ***\*2A\****. Critic 함수를 활용해 현재 상태와 다음 상태에 대한 가치 (V-value)를 계산하는 코드를 완성해주세요.
  - ***\*2B\****. Advantage 값을 계산하기 위해 현재 상태에 대한 기대 행동 가치 (Q-value)를 계산하는 코드를 완성해주세요
  - ***\*2C\****. Advantage 값을 활용해서 Actor를 학습시키기 위해, 목적 함수를 계산하는 코드를 완성해주세요.

### Tips!

1. PyTorch에서 정규 분포를 생성하는 코드는 아래와 같습니다.

```
# 평균이 a, 표준 편차가 b인 정규 분포를 생성
distribution = torch.distributions.Normal(a, b)Copy
```

1. PyTorch Tensor의 평균을 구할 때, `.mean()` 메서드를 활용할 수 있습니다.

```
# tensor 변수의 Tensor의 평균값을 계산한다
tensor_mea = tensor.mean()
```